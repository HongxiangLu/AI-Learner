[TOC]

# 第一章 人工智能的基础概念

人工智能是通过机器来模拟人类感知、认知、推理、决策的技术。在当下，人工智能的流行方法是机器学习，也就是通过学习来获得预测和判断的能力。机器学习需要数据，如果这些数据具有明确的学习目标（标签），就是监督学习；否则就是无监督学习。监督学习根据学习目标是否离散分为分类和回归；无监督学习中的一种叫做聚类，也就是把数据自动地分成几类；另一种叫做降维，也就是把复杂的数据简化。除此之外，第三种机器学习方法叫做强化学习，也就是让模型在环境中通过奖惩来学习。强化学习不需要一系列包含输入与预测的样本，它是在行动中学习。

# 第二章 分类

## 2.1 基础概念

人工智能系统学习的数据类型包括文本、图像、视频、音频等。分类就是根据数据的某些特征将其映射到离散的标签的过程。特征在计算机中以向量的方式表示。我们可以将特征向量表示在直角坐标系中，这些表示特征向量的点称为特征点，所在的坐标系称为特征空间。根据特征向量输出预测类别的函数叫做**分类器**。例如对于一些二维数据，可以使用一条平面上的直线将其分成两类。这个分类函数的参数（包括权重和偏置）叫做模型参数。

让人工智能系统通过数据自动调整模型参数的过程称为训练。在分类任务中，数据需要划分为训练集和测试集，它们本质上都包含特征和标签：

- 训练集：标签对模型可见，用于计算误差并更新参数。
- 测试集：标签对模型不可见，仅在模型做出预测后，用来评估模型的准确性。

训练过程所遵循的具体计算步骤，被称为学习算法或优化算法。

损失函数是量化模型预测值与真实标签之间差异的数学公式，是模型的「评分标准」：预测偏差越多，损失函数的值越大。优化是指通过算法不断调整模型参数，找到一组最佳参数，使得整个训练集上的总损失达到最小的过程。

## 2.2 线性二分类器：感知器算法

感知器是线性分类器训练算法的一种，基本原理是使用被误训练的数据来修正模型参数。算法步骤具体如下，其中数据集包含特征向量 $(x_1, x_2)$ 和对应的标签 $y$，标签 $y$ 的取值为 $\{+1, -1\}$。

1. 初始化：

   将权重$w_1, w_2$和偏置 $b$ 初始化为随机数，设定学习率 $\alpha$。

2. 迭代训练：

   对训练集中的每一条数据 $(x^{(i)}_1, x^{(i)}_2)$，执行以下步骤：

   - 计算预测值：$z = w_1 x_1 + w_2 x_2 + b$

   - 检查条件 $y \cdot z \le 0$ 是否成立：

     - 如果 $y \cdot z > 0$：说明预测符号与标签符号一致（分类正确），不做任何操作。
   - 如果 $y \cdot z \le 0$：说明分类错误（或者点正好在边界上），执行参数更新。

3. 参数更新：

   $$w_1 \leftarrow w_1 + \alpha \cdot y \cdot x_1$$

   $$w_2 \leftarrow w_2 + \alpha \cdot y \cdot x_2$$

   $$b \leftarrow b + \alpha \cdot y$$

4. 终止条件检查：

   重复步骤 2 和 3（这称为一个 Epoch），直到训练集中没有误分类点（即所有点都满足 $y \cdot z > 0$）。

> 疑问：参数更新公式为什么能够让直线朝着误分类样本的方向移动？需要直观解释。

## 2.3 线性二分类器：线性支持向量机算法

支持向量机与感知器一样，属于二分类器。 线性分类器是分类器中的一种，类似地，线性支持向量机也是支持向量机中的一种。使用权重 $\mathbf{w}$ 和偏置 $\mathbf{b}$ 定义一条直线之后，点到直线的距离可以用公式
$$
\frac{\|\mathbf{w}^T\mathbf{x} + b\|}{\|\mathbf{w}\|}
$$
表示。我们定义数据点 $\mathbf{x}$ 到决策边界（直线）的几何间隔为：
$$
\gamma = \frac{y \times (\mathbf{w}^T\mathbf{x} + b)}{\|\mathbf{w}\|}
$$
其中 $\gamma$ 是数据的真实标签。这样一来，如果数据点被正确分类，几何间隔等于点线距离；否则就是相反数。几何间隔越大，说明点离边界越远，直线越能清晰地分隔两种类别。我们将整个训练集中**最小**的几何间隔定义为数据集的间隔 $\beta$。支持向量机的核心思想是：**找到一条直线，使得这个最小间隔 $\beta$ 最大化**。

为了避免 $\mathbf{w}$ 和 $\mathbf{b}$ 的自由缩放，我们通过**数学约束（损失函数）**，令**两侧**离边界最近的数据点（即**支持向量**）满足 
$$
y\times(\mathbf{w}^T\mathbf{x}+b)=1
$$
。此时，最小间隔 $\beta$ 变成 $\frac{1}{\|w\|}$，模型的优化目标（损失函数的一部分）就变成了$\frac{\|\mathbf{w}\|^2}{2}$。

在数学上，线性支持向量机的损失函数通常写为：
$$
J(\mathbf{w},b)=\frac{\lambda}{2}\|\mathbf{w}\|^2+\frac{1}{N}\sum\max(0,1-y_i(w^Tx_i+b))
$$
其中后半部分保持数据点的几何间隔至少为 $1$。

## 2.4 训练和验证

模型在经过训练之后，需要在独立的数据集上进行评估。为了严谨地挑选最佳模型，我们通常将数据划分为三部分：训练集、验证集和测试集。

1. 我们使用**训练集**训练多个不同参数的模型。
2. 我们在**验证集**上对比这些模型的效果，选出表现最好的那个（这一步叫**模型选择**）。
3. 最后，我们将选出的模型在**测试集**上进行测试，得到这个模型在这个数据集上的最终表现。

在分类任务中，测试过程是将每一条测试数据的**预测标签**与**真实标签**进行比对。预测正确的样本数占总样本数的比例称为**准确率 (Accuracy)**。虽然准确率是最直观的指标，但在实际应用中，我们还需要结合数据的分布情况，综合考虑模型的泛化能力，才能决定是否将其部署到实际生活中。

当我们根据测试集的表现来优选模型或调整超参数时，测试集就不再是单纯的「裁判」，而变成了「教练」；因为它所包含的信息（即哪些参数在这批数据上表现好）通过你的筛选决策间接地「泄露」给了模型，导致模型针对这套特定的测试题进行了「过拟合」，使得测试结果虚高，无法真实反映模型在处理未来未知数据时的泛化能力。

## 2.5 多类别分类

**多分类 (Multi-class Classification)** 是二分类的推广。实现多分类主要有两种思路：

1. **组合策略：** 以 **OvR (One-vs-Rest)** 为例。假如数据有 $K$ 个类别，我们可以训练 $K$ 个**独立**的二分类器（例如「是否属于某一类」）。预测时，让这 $K$ 个分类器分别打分，得分最高的类别即为预测结果。
2. **直接预测策略：** 在现代深度学习中，更流行的方法是让模型末端直接输出 $K$ 个数值（称为 Logits）。然后，利用**Softmax 函数**（归一化指数函数）处理这些数值：
   - 它将数值转化为 **0 到 1 之间的概率**；
   - 它保证所有类别的概率之和为 **1**。

最终，概率值最大的那个类别就是模型的预测结果。

## 2.6 二分类在生活中的应用

二分类技术的一个典型应用是**人脸检测 (Face Detection)**。在手机拍照时，算法通常采用**滑动窗口 (Sliding Window)** 策略，将画面分割成无数个不同位置、不同大小的图像块。系统会对每一个图像块进行二分类判定（是否为人脸）。对于重叠度很高的多个候选框，算法会进行合并与筛选，最终保留最精准的人脸位置。

# 第三章 图像分类

## 3.1 基于手工特征的图像分类

### 3.1.1 计算机眼中的图像

在计算机内部，图像是以**三阶张量 (3rd-order Tensor)** 的形式存储的。张量的前两个维度分别代表图像的**高度 (Height)** 和 **宽度 (Width)**，对应像素的纵横坐标。第三个维度称为**通道数 (Channels)**：

- 当通道数为 **1** 时，图像为**灰度图**，每个像素由 0~255 的整数表示亮度。
- 当通道数为 **3** 时，图像为**彩色图 (RGB)**，每个像素包含红、绿、蓝三个分量，每个分量的值通常也是 0~255 的整数。

在这一领域发展的初期，人们手工设计了各种图像特征，用于描述图像的颜色、边缘、纹理等基本性质，结合机器学习技术解决物体识别和物体检测等实际问题。

### 3.1.2 利用卷积提取图像特征

既然图像以张量形式存在，我们通过**卷积**来提取其特征。

可以将卷积理解为「滑动窗口内积」。设有一个卷积核（即短向量 $a$）和一个输入信号（即长向量 $b$）。我们将卷积核覆盖在输入信号上，计算重叠部分的内积（对应元素相乘再求和），得到一个数值。然后将卷积核向后滑动一步，重复此过程。

- 输出长度公式为：$L_{out} = L_{in} - L_{kernel} + 1$（假设步长为 1 且无填充）。
- *注：严格数学定义中需先翻转卷积核，但在深度学习中通常省略此步，本质上使用的是「互相关」。*

在二维矩阵的卷积操作中，卷积核变成了一个矩阵（如 $3 \times 3$），在图像（大矩阵）上从左到右、从上到下通过滑动窗口计算内积。而在三维张量（或者说RGB图像，维度为$H \times W \times 3$）中，卷积核的深度通常与图像通道数匹配。计算时，卷积核的 3 个通道分别与图像的 3 个通道做矩阵运算，最后将所有结果**求和**，输出为**单通道**的二维特征图。

使用卷积操作可以提取图像的特征。在这个过程中，对应的权重矩阵称为**卷积核 (Kernel/Filter)**。 卷积核与原图片进行滑动卷积后，输出的结果被称为**特征图 (Feature Map)**。这张图上的高亮区域代表了原图中具备该特征的位置。

1. **竖直边缘检测：** 一个「左列为正、右列为负」的卷积核，实质上是在计算图像左右相邻像素的**颜色差值（梯度）**。如果某处存在竖直边界（左暗右亮），差值会很大，从而在特征图上凸显出竖直边缘。
2. 如果卷积核沿对角线方向**两侧分别为正和负**，则是为了检测图像中的**倾斜边缘**。只有当图像中存在一条斜线，使得斜线两侧的颜色反差很大（比如左上角是黑，右下角是白）时，正负相抵后的结果才会特别大。

## 3.2 基于深度神经网络的图像分类

### 3.2.1 深度神经网络的结构

2010年，李飞飞团队推动了 ImageNet 图像识别竞赛，涵盖 1000 个图像类别。在深度学习普及之前，基于手工特征的传统方法遭遇了瓶颈，错误率长期停留在 26% 以上。2012年，Hinton 的学生推出的 **AlexNet** 横空出世，将错误率断崖式降低了十个百分点。此后，深度学习统治了该领域。到了 2015 年，**ResNet** 首次超越了人类视觉识别的平均水平（约 5%）。

深度神经网络之所以具备强大的能力，是因为可以在复杂度降低的同时自动地从图像中提取有效特征。深度神经网络由多个顺序连接的层（Layer）组成。第一层一般接收图像等数据作为输入，通过特定的运算从中提取特征。随后的层以前一层提取出的特征作为输入，对其进行特定形式的变换，得到更加复杂和高级的特征。这种层次化的结构赋予了神经网络强大的特征提取能力。

AlexNet 是早期卷积神经网络（CNN）的代表，它包含**八个可学习层**：

- **5 个卷积层 (Conv)：** 用于提取图像特征。所有卷积层后都接有 **ReLU** 激活函数，解决了梯度消失问题。其中，第 1、2、5 个卷积层之后接有**最大池化层 (Max Pooling)**，用于降低维度，最终图像被转为 **4096** 维的特征向量。
- **3 个全连接层 (Fully Connected/Linear)：** 负责分类逻辑。前两层后分别跟有 **ReLU** 激活函数，并使用了 **Dropout** 技术来防止过拟合；最后一层输出 1000 维，对应 1000 个类别的得分。
- **输出端：** 最后接 **Softmax** 函数，将得分转化为概率分布。

### 3.2.2 深度神经网络的结构——卷积层

**卷积层**是深度神经网络在处理图像时常用的一种层。以卷积层为主体的神经网络称为卷积神经网络。顾名思义，卷积层使用卷积运算对原始图像或上一层的输出进行变换，输入格式为三阶张量（例如 $H_{in} \times W_{in} \times C_{in}$，其中 $C_{in}$ 是输入通道数，如 RGB 图像为 3）。

为了提取多种形式的特征，卷积层通常包含多个（例如 $K$ 个）不同的**卷积核 (Kernels/Filters)**。每个卷积核的深度都与输入的通道数 $C_{in}$ 相同，每个通道分别进行卷积运算。运算结果沿着深度方向求和，生成一个单通道的三阶张量。

将这些三阶张量作为不同的通道组合起来（在深度方向上堆叠），得到新的三阶张量输出。这个张量的通道数等于卷积核的个数，形状为 $H_{out} \times W_{out} \times K$。由于每个通道都是从图像中提取的一种特征， 我们也将这个三阶张量称为**特征图 (Feature Map)**。 

### 3.2.3 深度神经网络的结构——全连接层

输入图片在经过若干卷积层提取出特征图后，为了进一步整合信息并实现最终任务，数据需要通过全连接层。

**首先是形态转换：**全连接层会将三维特征图**拉平（Flatten）**成一维的特征向量。这一步打破了卷积层的空间结构，使全连接层可以一次性看到所有特征，从而通过权重来判断这些特征组合起来到底是什么。

**接着是核心计算：** 全连接层利用 $K$ 个维数相同的权重向量与输入向量进行**内积运算**（通常伴随偏置相加），得到 $K$ 个标量结果并拼接输出。这里的 $K$ 根据层级位置有不同含义：

- 作为**中间层**：$K$ 是超参数（神经元个数），用于学习数据的**隐式表达**；
- 作为**输出层**：$K$ 对应具体的**类别数量**。

最终，**全连接层实现了两个功能**：一是整合特征。卷积层关注的是「局部特征」，需要保留空间结构。而全连接层的任务是全局决策，把分散的特征综合起来考虑（这也是拉平的作用）。二是分类映射，将抽象的「特征空间」映射到目标要求的类别空间。 

### 3.2.4 深度神经网络的结构——归一化指数层

Softmax 层通常位于图像分类神经网络的**最末端**，接收一个长度等于类别数量的向量（**Logits**）作为输入，输出图像属于各个类别的概率。

**功能：**（1）**归一化 (Normalization)：** 将任意实数映射到 $[0, 1]$ 之间，并保证所有数值之和为 1，构成严谨的**概率分布**。（2）**指数放大 (Exponential)：** 公式中的 $e^x$ 会拉大数值之间的差距，让原本得分稍高的类别在概率上变得更加显著（强者恒强）。

**与 Hardmax 的区别**：Hardmax 只保留最大值，并且不可求导。而 **Softmax** 处处**可导**，能够保留所有类别的概率信息，可以用于神经网络的反向传播训练，指导模型优化。

### 3.2.5 深度神经网络的结构——非线性激活层

在深度神经网络中，非线性激活层通常紧跟在每个卷积层或全连接层之后。

**原因：**不管是卷积运算还是全连接层的运算，本质上都是线性变换，即 $y = Wx + b$。线性函数具有封闭性：多个线性函数的复合，依然是一个线性函数。如果在层与层之间不引入非线性机制，无论网络堆叠多深，其整体表达能力都等价于单层线性模型。这将导致深度神经网络退化，无法学习图像中复杂的特征。

**机制：** 激活层通过引入非线性函数（如 **ReLU** 或 **Sigmoid**），对特征图的每一个元素进行**逐元素变换**。这种非线性扭曲使得每次变换的效果得以保留，从而赋予了深度学习处理复杂现实问题的能力。

### 3.2.6 深度神经网络的结构——池化层

池化层通常紧跟在卷积层之后，用于压缩数据维度。

**目的：**一是通过减小特征图的分辨率（长和宽），大幅减少后续层的参数量和计算消耗。二是引入**平移不变性**，使模型对图像中目标的轻微位置偏移不敏感，提高鲁棒性。

**计算机制：**池化操作是**逐通道 (Channel-wise)** 进行的，深度保持不变。算法利用一个固定大小的窗口（如 $2 \times 2$）在特征图上滑动（通常步长与窗口大小相同，即**无重叠滑动**），取窗口内的**最大值**或**平均值**。

**通俗一点：**首先将特征图按照通道分开，得到若干个矩阵。将每个矩阵切割成大小相同的正方形区块，对每个区块取最大值或平均值，并将结果组合成新的矩阵。最后，将所有通道的结果按原顺序堆叠形成新的三维张量。

**类别：**最大池化（Max Pooling）取窗口内的最大值，能保留最显著的纹理特征（如最亮的像素、最强的边缘）。平均池化（Average Pooling）取窗口内的平均值，能保留背景信息，起到平滑作用。

### 3.2.7 人工神经网络的训练

**来源：**人工神经网络的设计灵感来源于生物神经网络。生物神经网络由数以亿计的神经元连接而成，通过神经元间的信号传递进行思维活动。类似地，特征图或特征向量中的每个元素也称为**神经元**，元素的值代表神经元对特定特征的**响应程度**（激活程度）。

**训练：**与分类器相似，深度神经网络同样需要训练才能发挥作用。**训练本质上就是寻找最佳参数的过程。**在神经网络里，卷积层中卷积核的元素值、全连接层中内积运算的系数（权重）和偏置都是参数。前文提到的线性二分类器只需几个参数，而在深度神经网络中，参数的数量级通常在数千万到数千亿之间。为了高效训练神经网络，需要反向传播算法。

**反向传播算法：**在图像分类任务中，每次我们将一幅图像输入网络，经过逐层计算得到预测的概率值。将预测结果与真实结果进行对比，如果不够理想，就从后向前依次修改神经网络的参数，使得神经网络能够对这个样本做出更准确的预测。具体流程为：前向传播➡️计算误差➡️反向传播➡️更新参数。

## 3.3 深度神经网络的发展与挑战

### 3.3.1 过拟合和欠拟合

**过拟合（High Variance）**指的是模型在**训练集**上得分很高，但在**验证集、测试集**上得分显著下降。原因是模型「脑容量」过大（参数太多），导致它不再是学习一般规律，而是**死记硬背**了训练数据中的噪声和细节。

**欠拟合（High Bias）**指的是模型在**训练集和测试集**上的表现都很差。原因是模型太简单（层数不够），或者训练时间太短，甚至连基本规律都没学会。

**解决方案（正则化 Regularization）：**为了在增加模型深度、提升表达能力的同时防止过拟合，常用以下手段：

- **权值衰减 ($L_2$ Regularization)：** 限制参数数值不要太大，防止模型过于敏感。
- **Dropout：** 训练时随机关闭部分神经元，防止神经元之间过度依赖。
- **数据增强：** 通过旋转、缩放图像增加数据多样性。
- **早停 (Early Stopping)：** 在验证集性能恶化前及时停止训练。

### 3.3.2 梯度消失

神经网络的训练依赖于**反向传播算法**，即通过**链式法则**将输出层的误差（Error）对参数求导，计算出梯度，从而指引参数更新。在深层网络中，如果使用 Sigmoid 等传统激活函数（导数小于 1），梯度在从后向前传递时，会经过连续的**矩阵乘法**，到达靠近输入层的浅层网络时，梯度几乎为零。结果是浅层网络的参数不再更新，导致网络无法提取底层特征，训练失败。

为了解决这一问题，深度学习领域发展出了三大核心技术：

- **更换激活函数：** 使用 **ReLU**（导数为 1）替代 Sigmoid，防止连乘衰减。
- **批归一化 (Batch Normalization, BN)：** 强行将每一层的输入拉回标准正态分布，保证梯度处在有效范围。
- **残差结构 (Residual Connection / Skip Connection)：** 通过跨层连接，让梯度可以直接跨层传导，即使网络深达上百层也能正常训练。

## 3.4 图像分类在日常生活中的应用——人脸识别

人脸识别是指从数字图像或者一帧视频中，「找到人脸」并「认出人脸」的过程。 其中「认出人脸」就是一种图像分类任务。人脸识别系统通常包含以下四个核心步骤：

1. **人脸检测——「看得到」：**从复杂的背景中定位人脸，输出人脸的坐标框和关键点信息。
2. **人脸对齐——「摆得正」：**根据检测到的关键点（如眼睛、嘴巴），利用仿射变换将人脸图像进行旋转、缩放和裁剪，使其变成一张标准的正脸图像，消除角度带来的干扰。
3. **特征提取——「看得懂」：**将对齐后的人脸输入深度神经网络。这里的特征通常**不是**「微笑」或「眼镜」等语义属性，而是一个**高维特征向量**（如 512 维的实数向量）。这个向量将人脸映射到高维空间中的一个点，要求**同一个人不同照片的距离尽可能近，不同人的距离尽可能远**。
4. **人脸比对——「跟谁像」：**计算现场人脸特征向量与数据库中人脸特征向量之间的**距离**（通常使用余弦相似度，Cosine Similarity）。
   - 如果用于**1:1 验证**（如手机解锁），通过判断相似度是否高于阈值来决定是否通过。
   - 如果用于**1:N 搜索**（如抓逃犯），则是找出数据库中相似度最高的那个人。

* **余弦相似度**：两个向量 $\mathbf{A}$ 和 $\mathbf{B}$ 在多维空间中夹角 $\theta$ 的余弦值 $\cos(\theta)$。它的核心意义是衡量方向是否一致，而不关注距离。

$$
\text{Cosine Similarity} = \cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|}
$$

# 第六章 聚类

## 2.1 基础概念

**定义：**训练数据没有标注信息的学习过程称为无监督学习。其中通过分析数据在特征空间中的聚集情况，可以将数据分为不同的组，这类任务称为**聚类（Clustering）**，这些组被称为**簇（Clusters）**。聚类是无监督学习中最基础的任务。

**目标：**聚类的终极目标是**「最大化簇内相似度，最小化簇间相似度」**，即同一个类别的样本距离尽可能近，不同类别的样本距离尽可能远。

聚类的重要假设是同一类别的数据在特征空间中相近。但并不是所有数据都满足这一假设。
